{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hMjxMhGxPS53"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn import model_selection\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "M2WkkxF8PfnP",
        "outputId": "c17c35f1-2324-4995-c062-2a5b701f891c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user id</th>\n",
              "      <th>movie id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>occupation</th>\n",
              "      <th>encoded age</th>\n",
              "      <th>encoded gender</th>\n",
              "      <th>encoded occupation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>196</td>\n",
              "      <td>242</td>\n",
              "      <td>3</td>\n",
              "      <td>881250949</td>\n",
              "      <td>49</td>\n",
              "      <td>M</td>\n",
              "      <td>writer</td>\n",
              "      <td>39</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>186</td>\n",
              "      <td>302</td>\n",
              "      <td>3</td>\n",
              "      <td>891717742</td>\n",
              "      <td>39</td>\n",
              "      <td>F</td>\n",
              "      <td>executive</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>377</td>\n",
              "      <td>1</td>\n",
              "      <td>878887116</td>\n",
              "      <td>25</td>\n",
              "      <td>M</td>\n",
              "      <td>writer</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>244</td>\n",
              "      <td>51</td>\n",
              "      <td>2</td>\n",
              "      <td>880606923</td>\n",
              "      <td>28</td>\n",
              "      <td>M</td>\n",
              "      <td>technician</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>166</td>\n",
              "      <td>346</td>\n",
              "      <td>1</td>\n",
              "      <td>886397596</td>\n",
              "      <td>47</td>\n",
              "      <td>M</td>\n",
              "      <td>educator</td>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user id  movie id  rating  timestamp  age gender  occupation  encoded age  \\\n",
              "0      196       242       3  881250949   49      M      writer           39   \n",
              "1      186       302       3  891717742   39      F   executive           29   \n",
              "2       22       377       1  878887116   25      M      writer           15   \n",
              "3      244        51       2  880606923   28      M  technician           18   \n",
              "4      166       346       1  886397596   47      M    educator           37   \n",
              "\n",
              "   encoded gender  encoded occupation  \n",
              "0               1                  20  \n",
              "1               0                   6  \n",
              "2               1                  20  \n",
              "3               1                  19  \n",
              "4               1                   3  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"ml-100k/u.data\",sep=\"\\t\", header=None)\n",
        "data1 = pd.read_csv(\"ml-100k/u.user\",sep=\"|\",header=None)\n",
        "data.columns = ['user id', 'movie id', 'rating', 'timestamp']\n",
        "data1.columns = ['user id','age','gender','occupation','zip code']\n",
        "data['movie id'].unique\n",
        "df1= pd.DataFrame(data1)\n",
        "df = pd.DataFrame(data)\n",
        "label_encoder = LabelEncoder()\n",
        "data = pd.merge(df,df1[['user id','age','gender','occupation']],on='user id',how='left')\n",
        "\n",
        "# data = merged\n",
        "data['encoded age']=label_encoder.fit_transform(data['age'])\n",
        "data['encoded gender']=label_encoder.fit_transform(data['gender'])\n",
        "data['encoded occupation']=label_encoder.fit_transform(data['occupation'])\n",
        "# print(data1['encoded age'][2])\n",
        "# print(data1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "FDZLbZ1_YfBU",
        "outputId": "05555c21-a41e-4dc6-845d-11bf71fe84e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie id</th>\n",
              "      <th>movie title</th>\n",
              "      <th>release date</th>\n",
              "      <th>video release date</th>\n",
              "      <th>IMDb URL</th>\n",
              "      <th>unknown</th>\n",
              "      <th>Action</th>\n",
              "      <th>Adventure</th>\n",
              "      <th>Animation</th>\n",
              "      <th>Children's</th>\n",
              "      <th>...</th>\n",
              "      <th>Fantasy</th>\n",
              "      <th>Film-Noir</th>\n",
              "      <th>Horror</th>\n",
              "      <th>Musical</th>\n",
              "      <th>Mystery</th>\n",
              "      <th>Romance</th>\n",
              "      <th>Sci-Fi</th>\n",
              "      <th>Thriller</th>\n",
              "      <th>War</th>\n",
              "      <th>Western</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>01-Jan-1995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>GoldenEye (1995)</td>\n",
              "      <td>01-Jan-1995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?GoldenEye%20(...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Four Rooms (1995)</td>\n",
              "      <td>01-Jan-1995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?Four%20Rooms%...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Get Shorty (1995)</td>\n",
              "      <td>01-Jan-1995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?Get%20Shorty%...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Copycat (1995)</td>\n",
              "      <td>01-Jan-1995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?Copycat%20(1995)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   movie id        movie title release date  video release date  \\\n",
              "0         1   Toy Story (1995)  01-Jan-1995                 NaN   \n",
              "1         2   GoldenEye (1995)  01-Jan-1995                 NaN   \n",
              "2         3  Four Rooms (1995)  01-Jan-1995                 NaN   \n",
              "3         4  Get Shorty (1995)  01-Jan-1995                 NaN   \n",
              "4         5     Copycat (1995)  01-Jan-1995                 NaN   \n",
              "\n",
              "                                            IMDb URL  unknown  Action  \\\n",
              "0  http://us.imdb.com/M/title-exact?Toy%20Story%2...        0       0   \n",
              "1  http://us.imdb.com/M/title-exact?GoldenEye%20(...        0       1   \n",
              "2  http://us.imdb.com/M/title-exact?Four%20Rooms%...        0       0   \n",
              "3  http://us.imdb.com/M/title-exact?Get%20Shorty%...        0       1   \n",
              "4  http://us.imdb.com/M/title-exact?Copycat%20(1995)        0       0   \n",
              "\n",
              "   Adventure  Animation  Children's  ...  Fantasy  Film-Noir  Horror  Musical  \\\n",
              "0          0          1           1  ...        0          0       0        0   \n",
              "1          1          0           0  ...        0          0       0        0   \n",
              "2          0          0           0  ...        0          0       0        0   \n",
              "3          0          0           0  ...        0          0       0        0   \n",
              "4          0          0           0  ...        0          0       0        0   \n",
              "\n",
              "   Mystery  Romance  Sci-Fi  Thriller  War  Western  \n",
              "0        0        0       0         0    0        0  \n",
              "1        0        0       0         1    0        0  \n",
              "2        0        0       0         1    0        0  \n",
              "3        0        0       0         0    0        0  \n",
              "4        0        0       0         1    0        0  \n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies = pd.read_csv(\"ml-100k/u.item\",\n",
        "                    sep=\"|\", encoding='latin-1', header=None)\n",
        "movies.columns = ['movie id', 'movie title' ,'release date','video release date', 'IMDb URL',\n",
        "                 'unknown', 'Action', 'Adventure', 'Animation', 'Children\\'s', 'Comedy',\n",
        "                 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n",
        "                 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
        "movies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "p_E-olF7l8RK",
        "outputId": "2355f2f8-6f82-45b9-d713-df2b647ae588"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Anna (1996)'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "map_id_movie = {}\n",
        "for id,row in movies.iterrows():\n",
        "  map_id_movie[row['movie id']] = row['movie title']\n",
        "map_id_movie[1398]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKncB8y0ZDpy",
        "outputId": "f32fd477-eafa-43d7-bb17-b3b4280afee3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of users: 943\n",
            "Number of movies: 1682\n",
            "Number of ages: 61\n",
            "Number of occupations: 21\n",
            "Number of genders: 2\n"
          ]
        }
      ],
      "source": [
        "num_users = data['user id'].nunique()\n",
        "# nusers = data1['user id'].nunique()\n",
        "num_age = data['encoded age'].nunique()\n",
        "num_occupation = data['encoded occupation'].nunique()\n",
        "num_gender = data['encoded gender'].nunique()\n",
        "num_movies = data['movie id'].nunique()\n",
        "print(\n",
        "    (f\"Number of users: {num_users}\\n\"\n",
        "    f\"Number of movies: {num_movies}\\n\"\n",
        "    f\"Number of ages: {num_age}\\n\"\n",
        "    f\"Number of occupations: {num_occupation}\\n\"\n",
        "    f\"Number of genders: {num_gender}\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "GcqUQ53BaGJ7",
        "outputId": "55a4bad5-52bb-4be2-dd3d-7e9e996b88d0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user id</th>\n",
              "      <th>movie id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>occupation</th>\n",
              "      <th>encoded age</th>\n",
              "      <th>encoded gender</th>\n",
              "      <th>encoded occupation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>24188</th>\n",
              "      <td>299</td>\n",
              "      <td>88</td>\n",
              "      <td>3</td>\n",
              "      <td>889502902</td>\n",
              "      <td>29</td>\n",
              "      <td>M</td>\n",
              "      <td>doctor</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14023</th>\n",
              "      <td>347</td>\n",
              "      <td>462</td>\n",
              "      <td>2</td>\n",
              "      <td>881654359</td>\n",
              "      <td>18</td>\n",
              "      <td>M</td>\n",
              "      <td>student</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20170</th>\n",
              "      <td>96</td>\n",
              "      <td>185</td>\n",
              "      <td>5</td>\n",
              "      <td>884403866</td>\n",
              "      <td>25</td>\n",
              "      <td>F</td>\n",
              "      <td>artist</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87853</th>\n",
              "      <td>880</td>\n",
              "      <td>302</td>\n",
              "      <td>5</td>\n",
              "      <td>880166451</td>\n",
              "      <td>13</td>\n",
              "      <td>M</td>\n",
              "      <td>student</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8174</th>\n",
              "      <td>177</td>\n",
              "      <td>289</td>\n",
              "      <td>2</td>\n",
              "      <td>880130534</td>\n",
              "      <td>20</td>\n",
              "      <td>M</td>\n",
              "      <td>programmer</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18733</th>\n",
              "      <td>43</td>\n",
              "      <td>215</td>\n",
              "      <td>5</td>\n",
              "      <td>883955467</td>\n",
              "      <td>29</td>\n",
              "      <td>F</td>\n",
              "      <td>librarian</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83494</th>\n",
              "      <td>860</td>\n",
              "      <td>516</td>\n",
              "      <td>3</td>\n",
              "      <td>885991040</td>\n",
              "      <td>70</td>\n",
              "      <td>F</td>\n",
              "      <td>retired</td>\n",
              "      <td>59</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36379</th>\n",
              "      <td>313</td>\n",
              "      <td>484</td>\n",
              "      <td>5</td>\n",
              "      <td>891016193</td>\n",
              "      <td>41</td>\n",
              "      <td>M</td>\n",
              "      <td>marketing</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17294</th>\n",
              "      <td>112</td>\n",
              "      <td>286</td>\n",
              "      <td>4</td>\n",
              "      <td>884992484</td>\n",
              "      <td>30</td>\n",
              "      <td>M</td>\n",
              "      <td>salesman</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71036</th>\n",
              "      <td>660</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>891405958</td>\n",
              "      <td>26</td>\n",
              "      <td>M</td>\n",
              "      <td>student</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       user id  movie id  rating  timestamp  age gender  occupation  \\\n",
              "24188      299        88       3  889502902   29      M      doctor   \n",
              "14023      347       462       2  881654359   18      M     student   \n",
              "20170       96       185       5  884403866   25      F      artist   \n",
              "87853      880       302       5  880166451   13      M     student   \n",
              "8174       177       289       2  880130534   20      M  programmer   \n",
              "...        ...       ...     ...        ...  ...    ...         ...   \n",
              "18733       43       215       5  883955467   29      F   librarian   \n",
              "83494      860       516       3  885991040   70      F     retired   \n",
              "36379      313       484       5  891016193   41      M   marketing   \n",
              "17294      112       286       4  884992484   30      M    salesman   \n",
              "71036      660         3       1  891405958   26      M     student   \n",
              "\n",
              "       encoded age  encoded gender  encoded occupation  \n",
              "24188           19               1                   2  \n",
              "14023            8               1                  18  \n",
              "20170           15               0                   1  \n",
              "87853            3               1                  18  \n",
              "8174            10               1                  14  \n",
              "...            ...             ...                 ...  \n",
              "18733           19               0                  10  \n",
              "83494           59               0                  15  \n",
              "36379           31               1                  11  \n",
              "17294           20               1                  16  \n",
              "71036           16               1                  18  \n",
              "\n",
              "[10000 rows x 10 columns]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train,data_test = model_selection.train_test_split(data, test_size = 0.1,random_state = 42,stratify = data['rating'])\n",
        "data_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "PjOZQ0f0ke_W"
      },
      "outputs": [],
      "source": [
        "class MF(nn.Module):\n",
        "  def __init__(self,users,movie_ids,ages,occupations,genders,embed_size):\n",
        "    super(MF,self).__init__()\n",
        "    self.user_embed = nn.Embedding(users,int(embed_size/4))\n",
        "    self.user_bias = nn.Embedding(users, 1)\n",
        "    self.age_embed = nn.Embedding(ages,int(embed_size/4))\n",
        "    self.age_bias = nn.Embedding(ages,1)\n",
        "    self.occupation_embed = nn.Embedding(occupations,int(embed_size/4))\n",
        "    self.occupation_bias = nn.Embedding(occupations,1)\n",
        "    self.gender_embed = nn.Embedding(genders,int(embed_size/4))\n",
        "    self.gender_bias = nn.Embedding(genders,1)\n",
        "    self.movie_embed = nn.Embedding(movie_ids,embed_size)\n",
        "    self.movie_bias = nn.Embedding(movie_ids,1)\n",
        "    self.user_embed.weight.data.uniform_(0,0.05)\n",
        "    self.age_embed.weight.data.uniform_(0,0.05)\n",
        "    self.occupation_embed.weight.data.uniform_(0,0.05)\n",
        "    self.gender_embed.weight.data.uniform_(0,0.05)\n",
        "    self.movie_embed.weight.data.uniform_(0,0.05)\n",
        "    self.user_bias.weight.data.uniform_(-0.01,0.01)\n",
        "    self.age_bias.weight.data.uniform_(-0.01,0.01)\n",
        "    self.occupation_bias.weight.data.uniform_(-0.01,0.01)\n",
        "    self.gender_bias.weight.data.uniform_(-0.01,0.01)\n",
        "    self.movie_bias.weight.data.uniform_(-0.01,0.01)\n",
        "\n",
        "\n",
        "  def forward(self,u,v,a,o,g):\n",
        "    # print(u.shape)\n",
        "    # print(v.shape)\n",
        "    U = self.user_embed(u)\n",
        "    # print(U.shape)\n",
        "    V = self.movie_embed(v)\n",
        "    # print(V.shape)\n",
        "    A = self.age_embed(a)\n",
        "    O = self.occupation_embed(o)\n",
        "    G = self.gender_embed(g)\n",
        "    b_u = self.user_bias(u).squeeze()\n",
        "    b_v = self.movie_bias(v).squeeze()\n",
        "    b_a = self.age_bias(a).squeeze()\n",
        "    b_o = self.occupation_bias(o).squeeze()\n",
        "    b_g = self.gender_bias(g).squeeze()\n",
        "    user_vec = torch.cat([U,A,O,G], dim=-1)\n",
        "    return (user_vec*V).sum(1) + b_u + b_v + b_a + b_o + b_g\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "mS_8zKl7phP5"
      },
      "outputs": [],
      "source": [
        "def train_epochs(model,lr,epochs,Nueralnet = False):\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  loss_fn = nn.MSELoss()\n",
        "  for t in range(epochs):\n",
        "    model.train()\n",
        "    users = torch.LongTensor(data_train['user id'].to_numpy()) # .cuda()\n",
        "    items = torch.LongTensor(data_train['movie id'].to_numpy()) #.cuda()\n",
        "    ages = torch.LongTensor(data_train['encoded age'].to_numpy())\n",
        "    occupations = torch.LongTensor(data_train['encoded occupation'].to_numpy())\n",
        "    genders = torch.LongTensor(data_train['encoded gender'].to_numpy())\n",
        "    ratings = torch.FloatTensor(data_train['rating'].to_numpy())\n",
        "    if Nueralnet:\n",
        "        ratings = ratings.unsqueeze(1)\n",
        "        y_pred = model(users,items)\n",
        "    else:\n",
        "        y_pred = model(users,items,ages,occupations,genders)\n",
        " #.cuda()\n",
        "    # y_pred = model(users,items,ages,occupations,genders)\n",
        "    loss_train = loss_fn(y_pred,ratings)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_users = torch.LongTensor(data_test['user id'].to_numpy())\n",
        "        test_items = torch.LongTensor(data_test['movie id'].to_numpy())\n",
        "        test_ages = torch.LongTensor(data_test['encoded age'].to_numpy())\n",
        "        test_occupations = torch.LongTensor(data_test['encoded occupation'].to_numpy())\n",
        "        test_genders = torch.LongTensor(data_test['encoded gender'].to_numpy())\n",
        "        test_ratings = torch.FloatTensor(data_test['rating'].to_numpy())\n",
        "        if Nueralnet:\n",
        "            test_ratings=test_ratings.unsqueeze(1)\n",
        "            pred_test = model(test_users,test_items)\n",
        "        else:\n",
        "            pred_test = model(test_users,test_items,test_ages,test_occupations,test_genders)\n",
        "        loss_test = loss_fn(pred_test, test_ratings)\n",
        "\n",
        "    print(f\"Epoch {t+1} -- Train loss: {loss_train:>7f} Test loss: {loss_test:>7f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hEqdQczIClM",
        "outputId": "eb9bc17c-ff79-4aae-ab02-7396165357d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 -- Train loss: 13.476962 Test loss: 10.746129\n",
            "Epoch 2 -- Train loss: 10.746518 Test loss: 7.512383\n",
            "Epoch 3 -- Train loss: 7.510556 Test loss: 4.300903\n",
            "Epoch 4 -- Train loss: 4.297201 Test loss: 1.906981\n",
            "Epoch 5 -- Train loss: 1.900885 Test loss: 1.292031\n",
            "Epoch 6 -- Train loss: 1.274644 Test loss: 2.538216\n",
            "Epoch 7 -- Train loss: 2.489333 Test loss: 3.593486\n",
            "Epoch 8 -- Train loss: 3.512446 Test loss: 3.341568\n",
            "Epoch 9 -- Train loss: 3.243386 Test loss: 2.389913\n",
            "Epoch 10 -- Train loss: 2.288691 Test loss: 1.499593\n",
            "Epoch 11 -- Train loss: 1.404516 Test loss: 1.044209\n",
            "Epoch 12 -- Train loss: 0.959463 Test loss: 1.024415\n",
            "Epoch 13 -- Train loss: 0.950346 Test loss: 1.253132\n",
            "Epoch 14 -- Train loss: 1.187907 Test loss: 1.532031\n",
            "Epoch 15 -- Train loss: 1.473022 Test loss: 1.733392\n",
            "Epoch 16 -- Train loss: 1.677929 Test loss: 1.804318\n",
            "Epoch 17 -- Train loss: 1.749957 Test loss: 1.743884\n",
            "Epoch 18 -- Train loss: 1.688439 Test loss: 1.582169\n",
            "Epoch 19 -- Train loss: 1.523695 Test loss: 1.366928\n",
            "Epoch 20 -- Train loss: 1.303721 Test loss: 1.154179\n",
            "Epoch 21 -- Train loss: 1.084849 Test loss: 0.997722\n",
            "Epoch 22 -- Train loss: 0.921329 Test loss: 0.934926\n",
            "Epoch 23 -- Train loss: 0.851172 Test loss: 0.971294\n",
            "Epoch 24 -- Train loss: 0.880711 Test loss: 1.073056\n",
            "Epoch 25 -- Train loss: 0.977086 Test loss: 1.179040\n",
            "Epoch 26 -- Train loss: 1.079900 Test loss: 1.231655\n",
            "Epoch 27 -- Train loss: 1.131960 Test loss: 1.207554\n",
            "Epoch 28 -- Train loss: 1.109809 Test loss: 1.125192\n",
            "Epoch 29 -- Train loss: 1.031347 Test loss: 1.026693\n",
            "Epoch 30 -- Train loss: 0.937893 Test loss: 0.951271\n",
            "Epoch 31 -- Train loss: 0.867824 Test loss: 0.918086\n",
            "Epoch 32 -- Train loss: 0.839575 Test loss: 0.924267\n",
            "Epoch 33 -- Train loss: 0.849748 Test loss: 0.953173\n",
            "Epoch 34 -- Train loss: 0.881374 Test loss: 0.984985\n",
            "Epoch 35 -- Train loss: 0.914482 Test loss: 1.004474\n",
            "Epoch 36 -- Train loss: 0.933812 Test loss: 1.004576\n",
            "Epoch 37 -- Train loss: 0.932360 Test loss: 0.986587\n",
            "Epoch 38 -- Train loss: 0.911546 Test loss: 0.958178\n",
            "Epoch 39 -- Train loss: 0.879232 Test loss: 0.930111\n",
            "Epoch 40 -- Train loss: 0.846438 Test loss: 0.912311\n",
            "Epoch 41 -- Train loss: 0.823431 Test loss: 0.910176\n",
            "Epoch 42 -- Train loss: 0.816024 Test loss: 0.922405\n",
            "Epoch 43 -- Train loss: 0.823382 Test loss: 0.941591\n",
            "Epoch 44 -- Train loss: 0.838543 Test loss: 0.957765\n",
            "Epoch 45 -- Train loss: 0.851891 Test loss: 0.963288\n",
            "Epoch 46 -- Train loss: 0.855964 Test loss: 0.956329\n",
            "Epoch 47 -- Train loss: 0.848890 Test loss: 0.940957\n",
            "Epoch 48 -- Train loss: 0.834506 Test loss: 0.924188\n",
            "Epoch 49 -- Train loss: 0.819454 Test loss: 0.912126\n",
            "Epoch 50 -- Train loss: 0.809422 Test loss: 0.907445\n",
            "Epoch 51 -- Train loss: 0.806688 Test loss: 0.909092\n",
            "Epoch 52 -- Train loss: 0.809883 Test loss: 0.913714\n",
            "Epoch 53 -- Train loss: 0.815430 Test loss: 0.917657\n",
            "Epoch 54 -- Train loss: 0.819548 Test loss: 0.918562\n",
            "Epoch 55 -- Train loss: 0.819838 Test loss: 0.916068\n",
            "Epoch 56 -- Train loss: 0.815969 Test loss: 0.911591\n",
            "Epoch 57 -- Train loss: 0.809446 Test loss: 0.907419\n",
            "Epoch 58 -- Train loss: 0.802695 Test loss: 0.905568\n",
            "Epoch 59 -- Train loss: 0.797908 Test loss: 0.906848\n",
            "Epoch 60 -- Train loss: 0.796085 Test loss: 0.910541\n",
            "Model trained\n"
          ]
        }
      ],
      "source": [
        "model = MF(num_users+1, num_movies+1,num_age+1,num_occupation+1,num_gender+1,embed_size=60)\n",
        "train_epochs(model,0.04,60)\n",
        "print(\"Model trained\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NueralNet(nn.Module):\n",
        "  def __init__(self,users,movie_ids,embed_size):\n",
        "    super(NueralNet,self).__init__()\n",
        "    self.user_embed = nn.Embedding(users,embed_size)\n",
        "    self.movie_embed = nn.Embedding(movie_ids,embed_size)\n",
        "    self.user_embed.weight.data.uniform_(0,0.05)\n",
        "    self.movie_embed.weight.data.uniform_(0,0.05)\n",
        "    self.network = nn.Sequential(\n",
        "            nn.Linear(2*embed_size,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "    self.output = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,u,v):\n",
        "    U = self.user_embed(u)\n",
        "    V = self.movie_embed(v)\n",
        "    vector = torch.cat([U,V], dim=-1)\n",
        "    out_temp = self.network(vector)\n",
        "    return self.output(out_temp)*5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 -- Train loss: 2.527740 Test loss: 2.507633\n",
            "Epoch 2 -- Train loss: 2.507829 Test loss: 2.487837\n",
            "Epoch 3 -- Train loss: 2.488024 Test loss: 2.468355\n",
            "Epoch 4 -- Train loss: 2.468524 Test loss: 2.449847\n",
            "Epoch 5 -- Train loss: 2.450004 Test loss: 2.431785\n",
            "Epoch 6 -- Train loss: 2.431925 Test loss: 2.413538\n",
            "Epoch 7 -- Train loss: 2.413655 Test loss: 2.395025\n",
            "Epoch 8 -- Train loss: 2.395121 Test loss: 2.376491\n",
            "Epoch 9 -- Train loss: 2.376558 Test loss: 2.359391\n",
            "Epoch 10 -- Train loss: 2.359435 Test loss: 2.342563\n",
            "Epoch 11 -- Train loss: 2.342593 Test loss: 2.325254\n",
            "Epoch 12 -- Train loss: 2.325260 Test loss: 2.307421\n",
            "Epoch 13 -- Train loss: 2.307398 Test loss: 2.289018\n",
            "Epoch 14 -- Train loss: 2.288962 Test loss: 2.269980\n",
            "Epoch 15 -- Train loss: 2.269884 Test loss: 2.250237\n",
            "Epoch 16 -- Train loss: 2.250093 Test loss: 2.229740\n",
            "Epoch 17 -- Train loss: 2.229543 Test loss: 2.208473\n",
            "Epoch 18 -- Train loss: 2.208212 Test loss: 2.186997\n",
            "Epoch 19 -- Train loss: 2.186671 Test loss: 2.166260\n",
            "Epoch 20 -- Train loss: 2.165871 Test loss: 2.144792\n",
            "Epoch 21 -- Train loss: 2.144333 Test loss: 2.122540\n",
            "Epoch 22 -- Train loss: 2.122002 Test loss: 2.099634\n",
            "Epoch 23 -- Train loss: 2.098994 Test loss: 2.075730\n",
            "Epoch 24 -- Train loss: 2.074971 Test loss: 2.050621\n",
            "Epoch 25 -- Train loss: 2.049721 Test loss: 2.024250\n",
            "Epoch 26 -- Train loss: 2.023188 Test loss: 1.996579\n",
            "Epoch 27 -- Train loss: 1.995331 Test loss: 1.967576\n",
            "Epoch 28 -- Train loss: 1.966117 Test loss: 1.936923\n",
            "Epoch 29 -- Train loss: 1.935220 Test loss: 1.902436\n",
            "Epoch 30 -- Train loss: 1.900402 Test loss: 1.865286\n",
            "Epoch 31 -- Train loss: 1.862872 Test loss: 1.826289\n",
            "Epoch 32 -- Train loss: 1.823452 Test loss: 1.785663\n",
            "Epoch 33 -- Train loss: 1.782349 Test loss: 1.743611\n",
            "Epoch 34 -- Train loss: 1.739769 Test loss: 1.700357\n",
            "Epoch 35 -- Train loss: 1.695940 Test loss: 1.656046\n",
            "Epoch 36 -- Train loss: 1.650991 Test loss: 1.610668\n",
            "Epoch 37 -- Train loss: 1.604898 Test loss: 1.564822\n",
            "Epoch 38 -- Train loss: 1.558255 Test loss: 1.518950\n",
            "Epoch 39 -- Train loss: 1.511505 Test loss: 1.473500\n",
            "Epoch 40 -- Train loss: 1.465088 Test loss: 1.428954\n",
            "Epoch 41 -- Train loss: 1.419486 Test loss: 1.385836\n",
            "Epoch 42 -- Train loss: 1.375222 Test loss: 1.344706\n",
            "Epoch 43 -- Train loss: 1.332852 Test loss: 1.306064\n",
            "Epoch 44 -- Train loss: 1.292881 Test loss: 1.270296\n",
            "Epoch 45 -- Train loss: 1.255692 Test loss: 1.237742\n",
            "Epoch 46 -- Train loss: 1.221621 Test loss: 1.208871\n",
            "Epoch 47 -- Train loss: 1.191151 Test loss: 1.183976\n",
            "Epoch 48 -- Train loss: 1.164586 Test loss: 1.163166\n",
            "Epoch 49 -- Train loss: 1.142050 Test loss: 1.146370\n",
            "Epoch 50 -- Train loss: 1.123486 Test loss: 1.133325\n",
            "Epoch 51 -- Train loss: 1.108648 Test loss: 1.123579\n",
            "Epoch 52 -- Train loss: 1.097098 Test loss: 1.116525\n",
            "Epoch 53 -- Train loss: 1.088257 Test loss: 1.111459\n",
            "Epoch 54 -- Train loss: 1.081437 Test loss: 1.107610\n",
            "Epoch 55 -- Train loss: 1.075901 Test loss: 1.104233\n",
            "Epoch 56 -- Train loss: 1.070924 Test loss: 1.100674\n",
            "Epoch 57 -- Train loss: 1.065872 Test loss: 1.096433\n",
            "Epoch 58 -- Train loss: 1.060262 Test loss: 1.091194\n",
            "Epoch 59 -- Train loss: 1.053785 Test loss: 1.084820\n",
            "Epoch 60 -- Train loss: 1.046309 Test loss: 1.077330\n",
            "Epoch 61 -- Train loss: 1.037864 Test loss: 1.068891\n",
            "Epoch 62 -- Train loss: 1.028607 Test loss: 1.059733\n",
            "Epoch 63 -- Train loss: 1.018773 Test loss: 1.050155\n",
            "Epoch 64 -- Train loss: 1.008642 Test loss: 1.040475\n",
            "Epoch 65 -- Train loss: 0.998491 Test loss: 1.030954\n",
            "Epoch 66 -- Train loss: 0.988585 Test loss: 1.021830\n",
            "Epoch 67 -- Train loss: 0.979159 Test loss: 1.013284\n",
            "Epoch 68 -- Train loss: 0.970369 Test loss: 1.005438\n",
            "Epoch 69 -- Train loss: 0.962304 Test loss: 0.998326\n",
            "Epoch 70 -- Train loss: 0.954981 Test loss: 0.991928\n",
            "Epoch 71 -- Train loss: 0.948361 Test loss: 0.986196\n",
            "Epoch 72 -- Train loss: 0.942371 Test loss: 0.981037\n",
            "Epoch 73 -- Train loss: 0.936904 Test loss: 0.976306\n",
            "Epoch 74 -- Train loss: 0.931835 Test loss: 0.971961\n",
            "Epoch 75 -- Train loss: 0.927143 Test loss: 0.967934\n",
            "Epoch 76 -- Train loss: 0.922676 Test loss: 0.964152\n",
            "Epoch 77 -- Train loss: 0.918411 Test loss: 0.960541\n",
            "Epoch 78 -- Train loss: 0.914291 Test loss: 0.957062\n",
            "Epoch 79 -- Train loss: 0.910274 Test loss: 0.953723\n",
            "Epoch 80 -- Train loss: 0.906358 Test loss: 0.950513\n",
            "Epoch 81 -- Train loss: 0.902539 Test loss: 0.947443\n",
            "Epoch 82 -- Train loss: 0.898823 Test loss: 0.944508\n",
            "Epoch 83 -- Train loss: 0.895220 Test loss: 0.941730\n",
            "Epoch 84 -- Train loss: 0.891740 Test loss: 0.939119\n",
            "Epoch 85 -- Train loss: 0.888392 Test loss: 0.936655\n",
            "Epoch 86 -- Train loss: 0.885192 Test loss: 0.934362\n",
            "Epoch 87 -- Train loss: 0.882154 Test loss: 0.932238\n",
            "Epoch 88 -- Train loss: 0.879281 Test loss: 0.930279\n",
            "Epoch 89 -- Train loss: 0.876565 Test loss: 0.928462\n",
            "Epoch 90 -- Train loss: 0.873994 Test loss: 0.926774\n",
            "Epoch 91 -- Train loss: 0.871559 Test loss: 0.925198\n",
            "Epoch 92 -- Train loss: 0.869253 Test loss: 0.923715\n",
            "Epoch 93 -- Train loss: 0.867059 Test loss: 0.922304\n",
            "Epoch 94 -- Train loss: 0.864973 Test loss: 0.920970\n",
            "Epoch 95 -- Train loss: 0.862997 Test loss: 0.919714\n",
            "Epoch 96 -- Train loss: 0.861106 Test loss: 0.918527\n",
            "Epoch 97 -- Train loss: 0.859289 Test loss: 0.917404\n",
            "Epoch 98 -- Train loss: 0.857545 Test loss: 0.916332\n",
            "Epoch 99 -- Train loss: 0.855864 Test loss: 0.915300\n",
            "Epoch 100 -- Train loss: 0.854240 Test loss: 0.914299\n",
            "Epoch 101 -- Train loss: 0.852671 Test loss: 0.913322\n",
            "Epoch 102 -- Train loss: 0.851152 Test loss: 0.912376\n",
            "Epoch 103 -- Train loss: 0.849680 Test loss: 0.911468\n",
            "Epoch 104 -- Train loss: 0.848254 Test loss: 0.910592\n",
            "Epoch 105 -- Train loss: 0.846874 Test loss: 0.909749\n",
            "Epoch 106 -- Train loss: 0.845538 Test loss: 0.908935\n",
            "Epoch 107 -- Train loss: 0.844248 Test loss: 0.908156\n",
            "Epoch 108 -- Train loss: 0.843008 Test loss: 0.907416\n",
            "Epoch 109 -- Train loss: 0.841820 Test loss: 0.906718\n",
            "Epoch 110 -- Train loss: 0.840686 Test loss: 0.906061\n",
            "Epoch 111 -- Train loss: 0.839604 Test loss: 0.905439\n",
            "Epoch 112 -- Train loss: 0.838578 Test loss: 0.904843\n",
            "Epoch 113 -- Train loss: 0.837605 Test loss: 0.904271\n",
            "Epoch 114 -- Train loss: 0.836687 Test loss: 0.903726\n",
            "Epoch 115 -- Train loss: 0.835824 Test loss: 0.903198\n",
            "Epoch 116 -- Train loss: 0.835018 Test loss: 0.902686\n",
            "Epoch 117 -- Train loss: 0.834267 Test loss: 0.902189\n",
            "Epoch 118 -- Train loss: 0.833572 Test loss: 0.901711\n",
            "Epoch 119 -- Train loss: 0.832929 Test loss: 0.901251\n",
            "Epoch 120 -- Train loss: 0.832336 Test loss: 0.900806\n",
            "Epoch 121 -- Train loss: 0.831789 Test loss: 0.900381\n",
            "Epoch 122 -- Train loss: 0.831283 Test loss: 0.899972\n",
            "Epoch 123 -- Train loss: 0.830812 Test loss: 0.899578\n",
            "Epoch 124 -- Train loss: 0.830371 Test loss: 0.899195\n",
            "Epoch 125 -- Train loss: 0.829955 Test loss: 0.898817\n",
            "Epoch 126 -- Train loss: 0.829563 Test loss: 0.898445\n",
            "Epoch 127 -- Train loss: 0.829191 Test loss: 0.898077\n",
            "Epoch 128 -- Train loss: 0.828835 Test loss: 0.897717\n",
            "Epoch 129 -- Train loss: 0.828493 Test loss: 0.897361\n",
            "Epoch 130 -- Train loss: 0.828164 Test loss: 0.897007\n",
            "Epoch 131 -- Train loss: 0.827847 Test loss: 0.896658\n",
            "Epoch 132 -- Train loss: 0.827541 Test loss: 0.896313\n",
            "Epoch 133 -- Train loss: 0.827247 Test loss: 0.895978\n",
            "Epoch 134 -- Train loss: 0.826964 Test loss: 0.895653\n",
            "Epoch 135 -- Train loss: 0.826690 Test loss: 0.895339\n",
            "Epoch 136 -- Train loss: 0.826425 Test loss: 0.895044\n",
            "Epoch 137 -- Train loss: 0.826166 Test loss: 0.894768\n",
            "Epoch 138 -- Train loss: 0.825913 Test loss: 0.894509\n",
            "Epoch 139 -- Train loss: 0.825663 Test loss: 0.894268\n",
            "Epoch 140 -- Train loss: 0.825417 Test loss: 0.894041\n",
            "Epoch 141 -- Train loss: 0.825173 Test loss: 0.893826\n",
            "Epoch 142 -- Train loss: 0.824931 Test loss: 0.893614\n",
            "Epoch 143 -- Train loss: 0.824690 Test loss: 0.893398\n",
            "Epoch 144 -- Train loss: 0.824446 Test loss: 0.893176\n",
            "Epoch 145 -- Train loss: 0.824199 Test loss: 0.892953\n",
            "Epoch 146 -- Train loss: 0.823949 Test loss: 0.892728\n",
            "Epoch 147 -- Train loss: 0.823691 Test loss: 0.892509\n",
            "Epoch 148 -- Train loss: 0.823431 Test loss: 0.892291\n",
            "Epoch 149 -- Train loss: 0.823164 Test loss: 0.892073\n",
            "Epoch 150 -- Train loss: 0.822889 Test loss: 0.891855\n",
            "Epoch 151 -- Train loss: 0.822605 Test loss: 0.891638\n",
            "Epoch 152 -- Train loss: 0.822310 Test loss: 0.891416\n",
            "Epoch 153 -- Train loss: 0.822002 Test loss: 0.891191\n",
            "Epoch 154 -- Train loss: 0.821675 Test loss: 0.890955\n",
            "Epoch 155 -- Train loss: 0.821332 Test loss: 0.890696\n",
            "Epoch 156 -- Train loss: 0.820976 Test loss: 0.890427\n",
            "Epoch 157 -- Train loss: 0.820608 Test loss: 0.890160\n",
            "Epoch 158 -- Train loss: 0.820230 Test loss: 0.889891\n",
            "Epoch 159 -- Train loss: 0.819834 Test loss: 0.889639\n",
            "Epoch 160 -- Train loss: 0.819401 Test loss: 0.889389\n",
            "Epoch 161 -- Train loss: 0.818942 Test loss: 0.889124\n",
            "Epoch 162 -- Train loss: 0.818466 Test loss: 0.888841\n",
            "Epoch 163 -- Train loss: 0.817966 Test loss: 0.888532\n",
            "Epoch 164 -- Train loss: 0.817440 Test loss: 0.888209\n",
            "Epoch 165 -- Train loss: 0.816884 Test loss: 0.887879\n",
            "Epoch 166 -- Train loss: 0.816298 Test loss: 0.887533\n",
            "Epoch 167 -- Train loss: 0.815680 Test loss: 0.887190\n",
            "Epoch 168 -- Train loss: 0.815032 Test loss: 0.886847\n",
            "Epoch 169 -- Train loss: 0.814357 Test loss: 0.886509\n",
            "Epoch 170 -- Train loss: 0.813659 Test loss: 0.886159\n"
          ]
        }
      ],
      "source": [
        "model_2 = NueralNet(num_users+1,num_movies+1,64)\n",
        "train_epochs(model_2,0.001,170,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeuPFwjcWA4v",
        "outputId": "a5725668-6383-47c6-8311-6836309e595e"
      },
      "outputs": [],
      "source": [
        "user_id = input(\"Enter the user_id for movie recommendations: \")\n",
        "movie_list = data[data['user id'] == int(user_id)]['movie id'].to_list()\n",
        "unseen_list = [int(x) for x in range(1,num_movies+1) if x not in movie_list]\n",
        "unseen_list = np.array(unseen_list)\n",
        "temp_x = torch.LongTensor(np.full(np.size(unseen_list),int(user_id)))\n",
        "temp_age = torch.LongTensor(np.full(np.size(unseen_list),int(data['encoded age'][int(user_id)-1])))\n",
        "temp_occ = torch.LongTensor(np.full(np.size(unseen_list),int(data['encoded occupation'][int(user_id)-1])))\n",
        "temp_gen = torch.LongTensor(np.full(np.size(unseen_list),int(data['encoded gender'][int(user_id)-1])))\n",
        "unseen_mov = torch.LongTensor(unseen_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjqlHAyD_KNw",
        "outputId": "8c980624-6ada-4dfe-af19-153030fccb33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------Recommended movies by MF--------------\n",
            "American Dream (1990) 9.839307\n",
            "Grosse Fatigue (1994) 8.996676\n",
            "Enfer, L' (1994) 8.4799\n",
            "Two or Three Things I Know About Her (1966) 8.185433\n",
            "Safe Passage (1994) 8.14035\n",
            "Ladybird Ladybird (1994) 7.870578\n",
            "Delta of Venus (1994) 7.8434196\n",
            "World of Apu, The (Apur Sansar) (1959) 7.757298\n",
            "Little City (1998) 7.6582255\n",
            "Legal Deceit (1997) 7.634438\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "predictions_MF = model(temp_x,unseen_mov,temp_age,temp_occ,temp_gen)\n",
        "sorted_indices = np.argsort(predictions_MF.detach().numpy())[::-1]\n",
        "ordered_movies = unseen_list[sorted_indices]\n",
        "ordered_ratings = predictions_MF.detach().numpy()[sorted_indices]\n",
        "print(\"-------------Recommended movies by MF--------------\")\n",
        "i=0\n",
        "for xyz in ordered_movies[:10]:\n",
        "  print(map_id_movie[xyz],ordered_ratings[i])\n",
        "  i=i+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "zsllIBos_cup"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------Recommended movies by NeuralNetworks--------------\n",
            "Safe Passage (1994) 4.869988\n",
            "Great Day in Harlem, A (1994) 4.848739\n",
            "Someone Else's America (1995) 4.8365054\n",
            "Santa with Muscles (1996) 4.8309402\n",
            "Entertaining Angels: The Dorothy Day Story (1996) 4.8307023\n",
            "Pather Panchali (1955) 4.829671\n",
            "Saint of Fort Washington, The (1993) 4.8254547\n",
            "The Deadly Cure (1996) 4.8253293\n",
            "Some Mother's Son (1996) 4.8247833\n",
            "Bitter Sugar (Azucar Amargo) (1996) 4.814805\n"
          ]
        }
      ],
      "source": [
        "model_2.eval()\n",
        "predictions_NN = model_2(temp_x,unseen_mov)\n",
        "predictions_NN=predictions_NN.squeeze()\n",
        "sorted_indices = np.argsort(predictions_NN.detach().numpy())[::-1]\n",
        "ordered_movies = unseen_list[sorted_indices]\n",
        "ordered_ratings = predictions_NN.detach().numpy()[sorted_indices]\n",
        "print(\"---------------Recommended movies by NeuralNetworks--------------\")\n",
        "i=0\n",
        "for xyz in ordered_movies[:10]:\n",
        "  print(map_id_movie[xyz],ordered_ratings[i])\n",
        "  i=i+1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
